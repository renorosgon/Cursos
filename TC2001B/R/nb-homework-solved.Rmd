---
title: "Naïve Bayes"
author: "MDS. René Rosado González"
output: 
  html_document:
    theme: paper
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

You will work with the `bes` dataset from *Llaudet, E., & Imai, K. (2022). Data Analysis for Social Science: A Friendly and Practical Introduction* book. This dataset contains the voting intention about Brexit, considering people age and education. Fill in the blanks in order to build a naive bayes classification model.

## Libraries

Load, and install if necessary, the `tidyverse` and `tidymodels` libraries, as well as the `discrim` package.

```{r libraries, warning = FALSE, message = FALSE}
# Install - load tidyverse                                                       
if(require(tidyverse) == FALSE){                                                
  install.packages('tidyverse')                                                 
  library(tidyverse)                                                            
}else{                                                                          
  library(tidyverse)                                                            
}
# Install - load tidymodels                                                       
if(require(tidymodels) == FALSE){                                                
  install.packages('tidymodels')                                                 
  library(tidymodels)                                                            
}else{                                                                          
  library(tidymodels)                                                            
}
# Install - load discrim                                                       
if(require(discrim) == FALSE){                                                
  install.packages('discrim')                                                 
  library(discrim)                                                            
}else{                                                                          
  library(discrim)                                                            
}
```

## Dataset

Load the `BES.csv` file.

```{r data, warning = FALSE, message = FALSE}
# Read csv
bes = read_csv('../data/BES.csv')
```

## Exploratory Data Analysis (EDA)

Perform an exploratory data analysis using the `glimpse` and `summary` functions.

```{r glimpse, warning = FALSE, message = FALSE}
## Tidyway
glimpse(bes)
```

```{r summary, warning = FALSE, message = FALSE}
# Summary
summary(bes)
```

Count the observations based on the `vote` and `leave` columns. What can you see?
```{r unique, warning = FALSE, message = FALSE}
# Count
bes %>% 
  count(vote, leave)
```

For the porpuse of this excersise we will focus only in those obsevations with a declared `leave` intention. This imply you must filter the data where `leave` is not an `NA`. 
```{r mutate, warning = FALSE, message = FALSE}
bes = bes %>% 
  # Filter the NA in the leave column
  filter(!is.na(leave)) %>% 
  # Transform the vote column into a factor with 'stay' and 'leave' as levels
  mutate(vote = factor(vote, levels = c('stay', 'leave')))

# Look at a summary
summary(bes)
```

Use the `ggdist` package to graph a boxplot and halfeye plot in the same graph. Set the `vote` column in the x axis, and `age` for the y axis. What pattern can you observe?

```{r ggplot, warning = FALSE, message = FALSE}
# Install - ggdist                                      
if(require(ggdist) == FALSE){                                                
  install.packages('ggdist') 
  library(ggdist)
} else {
  library(ggdist)
}

# Create a ggplot
ggplot(bes, aes(x = vote, y = age)) +
  # Add a stat_boxplot with a width of 0.25
  stat_boxplot(width = 0.25) + 
  # Add a stat_halfeye with a width of 0.5
  stat_halfeye(width = 0.5) + 
  # Add proper labels
  labs(
     # Modify the x and y axis labels to make them clearer
    x = 'Vote Intention',
    title = 'Older people were more likely to vote "leave"',
    subtitle = 'Age distribution',
    # Add the following caption
    caption = 'Own elaboration with data from Llaudet & Imai (2022)'
  ) +
  # Use a default theme
  theme_bw() +
  # Modify legend attributes using legend.attribute
  theme(
    axis.title.y = element_blank(),
    legend.background = element_blank(),
    legend.box.background = element_blank(),
    legend.key = element_rect(fill = "transparent"),
    legend.position = c(0.05, 0.9),
    legend.key.height = unit(0.5,'cm')
  ) 
```

Count the number of observations based on the `vote` and `education` columns. THen calculate the percentage of observations by voting preferences in each educational level, and create a column plot using this data. What pattern can you observe?

```{r ggplot2, warning = FALSE, message = FALSE}
bes %>% 
  # Count observations
  count(vote, education) %>%
  # Perform operations by group
  with_groups(
    # Use education as grouping variable
    .groups = education,
    # Set the mutate operation
    mutate,
    # calculate the percentage 
    pct = 100 * n/sum(n)
  ) %>% 
  # Create a ggplot
  ggplot(aes(x = education, y = pct)) +
  # Color by vote
  geom_col(aes(fill = vote)) + 
  # Add proper labels
  labs(
     # Modify the x and y axis labels to make them clearer
    x = 'Education Level',
    title = 'Less educated people were more likely to vote "leave"',
    subtitle = 'Share by educational level',
    # Add the following caption
    caption = 'Own elaboration with data from Llaudet & Imai (2022)',
    col = 'Voted'
  ) +
  # Use a default theme
  theme_bw() +
  # Modify legend attributes using legend.attribute
  theme(
    axis.title.y = element_blank(),
    legend.background = element_blank(),
    legend.box.background = element_blank(),
    legend.key = element_rect(fill = "transparent"),
    legend.key.height = unit(0.5,'cm')
  ) 
```

## Naive Bayes Classification

Create a model specification for the naive bayes using the `tidymodels` framework.

```{r model, warning = FALSE, message = FALSE}
nb_model = naive_Bayes() %>% 
  # Set the engine
  set_engine("klaR")  %>% 
  # Set the mode
  set_mode("classification") %>% 
  # Set hyperparameters to tune
  set_args(
    Laplace = tune(),
    smoothness = tune()
    )
```

Make an initial data split with 75% of the observations in the training set. To get consistent results lets set a random seed of `123`.

```{r splitting, warning = FALSE, message = FALSE}
# Data splitting for samples
set.seed(123)
bes_split = initial_split(
  # Data to split
  data = bes,
  # Proportions
  prop = 0.75,
  # Variable of interest
  strata = vote
)

# Get training and testing samples
bes_training = training(bes_split)
bes_testing = testing(bes_split)
```

Create 5 cross validation folds from the training set.

```{r folds, warning = FALSE, message = FALSE}
# Create cross-validation folds
set.seed(123)
bes_folds = vfold_cv(bes_training, v = 5)
```

Create the following recipe to predict the `vote` in function of age and education.

```{r recipe, warning = FALSE, message = FALSE}
# Create a recipe
recipe = recipe(
  # Set a formula
  formula = vote ~ age + education, 
  # Set input data
  data = bes_training
) %>% 
  # Impute missing values in the education column using knn
  step_impute_knn(education) %>% 
  # Transform number to factor in the education column
  step_num2factor(education, levels = as.character(c(1:5))) 
```

Make a workflow object with the recipe and model you just created.

```{r workflow, warning = FALSE, message = FALSE}
# Create a workflow
nb_workflow = workflow() %>% 
  # Add your recipe
  add_recipe(recipe) %>% 
  # Add your model
  add_model(nb_model)
```

Perform hyperparameter tuning on the workflow you just created using a `grid` of size 15.

```{r tuning, warning = FALSE, message = FALSE}
# Get parameters to tune
nb_parameters = nb_workflow %>% 
  extract_parameter_set_dials() 

# Create a tune grid
set.seed(123)
nb_tuning = tune_grid(
  object = nb_workflow,
  resamples = bes_folds,
  param_info = nb_parameters,
  metrics = metric_set(yardstick::accuracy),
  control = control_grid(verbose = TRUE),
  grid = 15
  )
```

Collect the tuning metrics and select the best model based on the `accuracy` metric.

```{r best_model, warning = FALSE, message = FALSE}
# Collegt metrics
tuning_metrics = nb_tuning %>%
  collect_metrics() %>% 
  # Arrange form higher mean to lower mean
  arrange(desc(mean))

# Select the best model
best_nb = nb_tuning %>%
  select_best("accuracy")

# Print the best model
print(best_nb)
```

Finalize your workflow and make the last fit to collect the performance of the model with the testing sample. Is the machine actually learning something?

```{r finalize, warning = FALSE, message = FALSE}
# Finalize the workflow with the best model
final_nb = nb_workflow %>% 
  finalize_workflow(best_nb)

# Make the last fit
results_nb = final_nb %>% 
  last_fit(
    split = bes_split,
    metrics = metric_set(yardstick::accuracy)
    )

# Collect testing metrics
metrics_nb = results_nb %>% 
  collect_metrics()

print(metrics_nb)
```
```{r conf_mat, warning = FALSE, message = FALSE}
# Print teh confussion matrix
results_nb %>% 
  collect_predictions() %>% 
  conf_mat(truth = vote, estimate = .pred_class)
```

Make predictions on the full dataset

```{r decision_boundary, warning = FALSE, message = FALSE}
# Extract the trained model
nb_trained = results_nb %>% 
  extract_fit_parsnip() 

# Make the predictions
predictions_tb =  augment(nb_trained, recipe %>% prep %>% bake(new_data = bes))
```

Create a plot to asses the relationship betweent the probability of voting 'leave' against the age
```{r assess, warning = FALSE, message = FALSE}
# Plot decision boundaries
ggplot(predictions_tb, aes(x = age)) +
  # Add the observations
  geom_point(aes(y = .pred_leave, col = education)) + 
  # Add proper labels
  labs(
    col = 'Education level',
     # Modify the x and y axis labels to make them clearer
    x = 'Age',
    title = 'Older people were more likely to vote "leave"',
    subtitle = 'Probability of voting "leave"',
    # Add the following caption
    caption = 'Own elaboration with data from Llaudet & Imai (2022)'
  ) +
  # Use a default theme
  theme_bw() +
  # Modify legend attributes using legend.attribute
  theme(
    axis.title.y = element_blank(),
    legend.background = element_blank(),
    legend.box.background = element_blank(),
    legend.key = element_rect(fill = "transparent"),
    legend.position = c(0.05, 0.9),
    legend.key.height = unit(0.5,'cm')
  ) 
```
